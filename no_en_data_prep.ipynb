{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47af274-ec2e-461c-bb78-2fb26e7b1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import random\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e057d88e-607e-419b-a3a9-9579bdcc8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_alphabets= r\"(?:(?![ა-ჰ])\\p{L})\" # this actually includes all the other languages too. only works with regex not re\n",
    "alphabets= \"([ა-ჰ])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = r\"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = r\"(\\w[.]\\w[.](?:\\w[.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"(\\d)\"\n",
    "hex_encoded_chars = r\"\\\\x..\" # \\x41\n",
    "url_encoded_chars = r\"\\&\\w+\\;\" # &amp;\n",
    "\n",
    "geo_bc = \"ძვ[.]წ[.]\"\n",
    "url_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
    "\n",
    "# <stop> will be an actual sentence splitting token\n",
    "# <sep> will just be a dot\n",
    "def split_into_sentences_and_remove_en(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    # remove patterns which will be hard to remove after all english is removed\n",
    "    text = re.sub(prefixes,\"\",text) # prefix\n",
    "    text = re.sub(url_pattern,\"\",text) # full url\n",
    "    text = re.sub(websites,\"\",text) # .tld\n",
    "    text = re.sub(hex_encoded_chars,\"\",text) # \\x41\n",
    "    text = re.sub(url_encoded_chars,\"\",text) # &amp;\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"\")\n",
    "    \n",
    "    text = regex.sub(\"\\s\" + en_alphabets + \"[.] \",\" \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = regex.sub(en_alphabets + \"[.]\" + en_alphabets + \"[.]\" + en_alphabets + \"[.]\",\" \",text)\n",
    "    text = regex.sub(en_alphabets + \"[.]\" + en_alphabets + \"[.]\",\" \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\" \",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \",text)\n",
    "    text = regex.sub(en_alphabets,\"\",text)\n",
    "    \n",
    "    # eng removing done\n",
    "    \n",
    "    # dates/times\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(r\"(\\d+)\\.(\\d+)\\.(\\d+)\",r\"\\1<prd>\\2<prd>\\3\",text) # dates\n",
    "    text = re.sub(geo_bc,\"ძვ<prd>წ<prd>\",text)\n",
    "    \n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"!..\" in text: text = text.replace(\"!..\",\"!<prd><prd>\")\n",
    "    \n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text) # თ. თოფურია\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = text.replace(\"„\",\"\\\"\")\n",
    "    text = text.replace(\"”\",\"\\\"\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    fake_dot = \"․\"\n",
    "    text = text.replace(\"<prd>\",fake_dot) # FAKE DOT THIS IS NOT ACTUAL DOT\n",
    "    assert fake_dot != \".\"\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c58a188-20ed-4cfa-bc32-a9210883246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20․20․2020.', 'დღეს არის 2020 წელი ძვ․წ․ ჰმჰმ.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences_and_remove_en('20.20.2020.  დღეს არის 2020 წელი ძვ.წ. ჰმჰმ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ad64d4-4fff-4f07-9653-33e8c876d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20․20․2020.', 'დღეს არის 2020 წელი ძვ․წ․ ჰმჰმ.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences_and_remove_en('20.20.2020.  დღეს არის 2020 წელი ძვ.წ. ჰმჰმ.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af06ebb-3e73-4763-a0fe-3a42696ca93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['შ․პ․ს․ მაგარი რამეა.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences_and_remove_en('შ.პ.ს. მაგარი რამეა.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29a89fc-1d92-4967-b804-41f20b8f4e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences_and_remove_en('hi this is so cool.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b96101-3177-4b94-84ac-65d0d78c4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "p_train = 0.7\n",
    "p_test = 0.2\n",
    "p_valid = 0.1\n",
    "total_sentences = 0\n",
    "\n",
    "with open('./no_en_data/ka_nse_train.txt', 'w') as f_train:\n",
    "    with open('./no_en_data/ka_nse_test.txt', 'w') as f_test:\n",
    "        with open('./no_en_data/ka_nse_valid.txt', 'w') as f_valid:\n",
    "            def process_data_file(path: str):\n",
    "                global total_sentences\n",
    "                num_docs = 0\n",
    "                with open(path, 'r') as input_file:\n",
    "                    cur_doc = ''\n",
    "                    for line in input_file:\n",
    "                        cur_doc += line\n",
    "                        if line.strip() == '': # assumption: document ends on an empty line\n",
    "                            doc_sentences = split_into_sentences_and_remove_en(cur_doc)\n",
    "                            if len(doc_sentences) == 0:\n",
    "                                continue\n",
    "\n",
    "                            r = random.random()\n",
    "                            if r < p_train:\n",
    "                                f = f_train\n",
    "                            elif r < p_train+p_test:\n",
    "                                f = f_test\n",
    "                            else:\n",
    "                                f = f_valid\n",
    "                            for s in doc_sentences:\n",
    "                                if len(s)==1:\n",
    "                                    continue\n",
    "                                f.write(s)\n",
    "                                total_sentences+=1\n",
    "                                f.write('\\n')\n",
    "                            f.write('\\n')\n",
    "                            \n",
    "                            if total_sentences >= 1_000_000:\n",
    "                                break\n",
    "                            cur_doc = ''\n",
    "                            num_docs+=1    \n",
    "                            if num_docs%10000==0:\n",
    "                                print(num_docs)\n",
    "            process_data_file('./data/უცხოელი_მწერლები_234.txt')\n",
    "            process_data_file('./data/ka.txt')\n",
    "import os\n",
    "for tp in ['train', 'test', 'valid']:\n",
    "    os.system(f\"sed -i -e '/./b' -e :n -e 'N;s/\\\\n$//;tn' ./no_en_data/ka_nse_{tp}.txt\")\n",
    "    # remove last line\n",
    "    os.system(f\"sed -i '$ d' ./no_en_data/ka_nse_{tp}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e078a6c-66d1-429a-8e37-2165a18ebbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./no_en_data/ka_nse_complete.txt', 'w') as f_train:\n",
    "    def process_data_file(path: str):\n",
    "        global total_sentences\n",
    "        num_docs = 0\n",
    "        with open(path, 'r') as input_file:\n",
    "            cur_doc = ''\n",
    "            for line in input_file:\n",
    "                cur_doc += line\n",
    "                if line.strip() == '': # assumption: document ends on an empty line\n",
    "                    doc_sentences = split_into_sentences_and_remove_en(cur_doc)\n",
    "                    if len(doc_sentences) == 0:\n",
    "                        continue\n",
    "\n",
    "                    f = f_train\n",
    "                    for s in doc_sentences:\n",
    "                        if len(s)==1:\n",
    "                            continue\n",
    "                        f.write(s)\n",
    "                        f.write('\\n')\n",
    "                    f.write('\\n')\n",
    "\n",
    "                    cur_doc = ''\n",
    "                    num_docs+=1    \n",
    "                    if num_docs%10000==0:\n",
    "                        print(num_docs)\n",
    "    process_data_file('./data/უცხოელი_მწერლები_234.txt')\n",
    "    process_data_file('./data/ka.txt')\n",
    "import os\n",
    "#for tp in ['train', 'test', 'valid']:\n",
    "os.system(f\"sed -i -e '/./b' -e :n -e 'N;s/\\\\n$//;tn' ./no_en_data/ka_nse_complete.txt\")\n",
    "# remove last line\n",
    "os.system(f\"sed -i '$ d' ./no_en_data/ka_nse_complete.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eec766-14e7-4c0f-a85b-96bbf4c13724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
