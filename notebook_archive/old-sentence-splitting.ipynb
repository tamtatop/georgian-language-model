{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47af274-ec2e-461c-bb78-2fb26e7b1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import re\n",
    "import numpy as np\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f937c16-c9f8-4d87-9231-79061e9e5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(container):\n",
    "    for c in container:\n",
    "        for i in c:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bc0247-691d-4e01-a125-a359d2beeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "def get_data(path_to_ka_data: str) -> List[str]:\n",
    "    f = open(path_to_ka_data)\n",
    "    N_DATA_WORDS = 10_000_000\n",
    "    data = f.read(N_DATA_WORDS*10)\n",
    "    f.close()\n",
    "    lines = data.splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e057d88e-607e-419b-a3a9-9579bdcc8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"(\\w)\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = r\"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = r\"(\\w[.]\\w[.](?:\\w[.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"(\\d)\"\n",
    "geo_bc = \"ძვ[.]წ[.]\"\n",
    "\n",
    "# <stop> will be an actual sentence splitting token\n",
    "# <sep> will just be a dot\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text) # prefix\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text) # websites\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(r\"(\\d+)\\.(\\d+)\\.(\\d+)\",r\"\\1<prd>\\2<prd>\\3\",text) # dates\n",
    "    text = re.sub(geo_bc,\"ძვ<prd>წ<prd>\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c58a188-20ed-4cfa-bc32-a9210883246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split_into_sentences('20.20.2020.  დღეს არის 2020 წელი ძვ.წ. ჰმჰმ.') == ['20.20.2020.', 'დღეს არის 2020 წელი ძვ.წ. ჰმჰმ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af06ebb-3e73-4763-a0fe-3a42696ca93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split_into_sentences('შ.პ.ს. მაგარი რამეა.') == ['შ.პ.ს. მაგარი რამეა.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29a89fc-1d92-4967-b804-41f20b8f4e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi this is so cool.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences('hi this is so cool.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "431c57c9-6d9d-41a0-9444-98d69ce6d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_data = get_data('./data/ka.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b96101-3177-4b94-84ac-65d0d78c4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/უცხოელი_მწერლები_234.txt', 'r') as input_file:\n",
    "    with open('./data/ka_nse_test.txt', 'w') as f:\n",
    "        for doc in input_file:\n",
    "            for s in split_into_sentences(doc):\n",
    "                f.write(s)\n",
    "                f.write('\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab81c2-17c0-4516-b682-f7c78a651efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a6fc7-5728-419e-bf80-0835d3270901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e7ebd-d0d7-465d-baff-86f650d288e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b4d4d-b440-4904-805a-4e648d59f253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe15f5a-c9b6-4e09-819a-9f3ab65a22dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b610c9-46a0-4f69-8807-80b58b370075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e410ff66-b029-466f-b030-670f115394c4",
   "metadata": {},
   "source": [
    "data = list(map(tokenizer, original_data))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "722a2188-82ae-48b7-93f4-f58206f9936a",
   "metadata": {},
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c449766-14d5-417b-b89c-3488d38d56a3",
   "metadata": {},
   "source": [
    "# Take in tokens, return sentences\n",
    "def basic_sentence_splitter(tokens: List[str]) -> List[List[str]]:\n",
    "    sentences = []\n",
    "    cur = []\n",
    "    for tok in tokens:\n",
    "        cur.append(tok)\n",
    "        if tok in ['.', '!', '?']:\n",
    "            sentences.append(cur)\n",
    "            cur = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03b98ddd-c85a-4d3d-be7f-3c2cb80e85b3",
   "metadata": {},
   "source": [
    "# Take in tokens, return sentences\n",
    "def sentence_splitter_bigger_than_one(tokens: List[str]) -> List[List[str]]:\n",
    "    sentences = []\n",
    "    cur = []\n",
    "    cnt_since_last_dot = 0\n",
    "    for tok in tokens:\n",
    "        cur.append(tok)\n",
    "        cnt_since_last_dot += 1\n",
    "        if tok in ['.', '!', '?']:\n",
    "            if cnt_since_last_dot > 2: # sentence should be longer than 1(2 because . counts as 1 too)\n",
    "                sentences.append(cur)\n",
    "                cur = []\n",
    "            cnt_since_last_dot = 0\n",
    "    if len(cur) > 0:\n",
    "        sentences.append(cur)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8023438-a23c-42ef-8898-b440dd806225",
   "metadata": {},
   "source": [
    "# CHANGE THIS TO CHANGE SENTENCE SPLITTER\n",
    "sentence_splitter = sentence_splitter_bigger_than_one"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb652143-fe4b-446f-8af8-65c15be14583",
   "metadata": {},
   "source": [
    "sentence_splitter(tokenizer('20.20.2020 მაგარი თარიღია. ძვ.წ. 2020 წელიც მაგარი იყო.'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11994e49-7356-49e8-a40c-f45fd1349acf",
   "metadata": {},
   "source": [
    "sentences = list(flatten(map(sentence_splitter, data)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e3aba75-21a2-4341-b601-515ceccf566e",
   "metadata": {},
   "source": [
    "f=open('./data/nose.txt', 'w')\n",
    "for s in sentences:\n",
    "    #print(s)\n",
    "    s=s[:40]\n",
    "    f.write(' '.join(s))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "158d84a4-ef32-46ba-a4f1-3396b01b695b",
   "metadata": {},
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "038b538a-544e-4637-9d48-5c71a8b51f06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geolm",
   "language": "python",
   "name": "conda-env-geolm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
