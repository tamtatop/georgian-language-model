{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bba9607-f1a1-4177-b4e1-caa93ba3825d",
   "metadata": {},
   "source": [
    "# Reading and tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a595dd8e-a2fc-49a7-b7d7-a15d2a050a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/khokho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from typing import Iterator, List\n",
    "from nltk import RegexpTokenizer, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re\n",
    "_patterns = [r\"<br \\/>\", r\",\", r\"\\(\", r\"\\)\", r\"\\„\", r\"\\“\"]\n",
    "\n",
    "_replacements = [\" \", \" , \", \" ( \", \" ) \", \" \\\" \", \" \\\" \"]\n",
    "\n",
    "_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
    "\n",
    "\n",
    "def generate_tokenized_sentences(path: Path, paragraphs=1_000_000) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Tokenize paragraphs into sentences, and sentences into words.\n",
    "    :param paragraph: text of paragraph\n",
    "    \"\"\"\n",
    "    word_tokenizer = RegexpTokenizer(r'\\b\\d+[/\\.]\\d+[/\\.]\\d+|ძვ\\.წ\\.|ახ\\.წ\\.|ე\\.წ\\.|ა\\.შ\\.|ე\\.ი\\.|[-\\w]+|[-!\\\".#$%&\\'()*+,/:;<=>?@\\[\\\\\\]^_`{|}~„“+]+')\n",
    "\n",
    "    # for sentence in sent_tokenize(paragraph):\n",
    "    with open(path, 'r') as file:\n",
    "        for i, paragraph in enumerate(file):\n",
    "            if i >= paragraphs:\n",
    "                break\n",
    "                \n",
    "            paragraph = paragraph.lower()\n",
    "            for pattern_re, replaced_str in _patterns_dict:\n",
    "                paragraph = pattern_re.sub(replaced_str, paragraph)\n",
    "\n",
    "            tokenized_sentence = word_tokenizer.tokenize(paragraph)\n",
    "            if tokenized_sentence:\n",
    "                start = 0\n",
    "                for i, tkn in enumerate(tokenized_sentence):\n",
    "                    if tkn in ('.', '!', '?'):\n",
    "                        yield tokenized_sentence[start: i+1]\n",
    "                        start = i+1\n",
    "\n",
    "                if start < len(tokenized_sentence):\n",
    "                    yield tokenized_sentence[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5a958f-73eb-4a82-bfe9-11fb3119d2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ძეგლთა',\n",
       " 'დაცვის',\n",
       " 'ეროვნულ',\n",
       " 'სააგენტოში',\n",
       " 'აცხადებენ',\n",
       " ',',\n",
       " 'რომ',\n",
       " 'მათ',\n",
       " 'მიერ',\n",
       " '\"',\n",
       " 'სამხრეთის',\n",
       " 'კარიბჭესთვის',\n",
       " '\"',\n",
       " 'მოწოდებული',\n",
       " 'ინფორმაცია',\n",
       " ',',\n",
       " 'იმის',\n",
       " 'შესახებ',\n",
       " 'რომ',\n",
       " 'ახალციხის',\n",
       " 'არქივის',\n",
       " 'ეზოში',\n",
       " 'აღმოჩენილი',\n",
       " 'შენობა',\n",
       " 'საკულტო',\n",
       " 'ნაგებობა',\n",
       " 'იყო',\n",
       " 'არასწორია',\n",
       " 'და',\n",
       " 'საბოლოოდ',\n",
       " 'დადგინდა',\n",
       " ',',\n",
       " 'რომ',\n",
       " 'აქ',\n",
       " 'ტრადიციული',\n",
       " 'მესხური',\n",
       " 'საცხოვრებელი',\n",
       " 'იყო',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "temp_data_path = Path('data/ka_nse_mil.txt')\n",
    "\n",
    "tokenized_sentences = [sent for sent in generate_tokenized_sentences(temp_data_path)]\n",
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f7490-69e5-44a3-be29-13cad9cb7380",
   "metadata": {},
   "source": [
    "# Training the n-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ac1cb4e-41fb-466e-927f-a72784c21409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.util import everygrams\n",
    "# from nltk.lm.preprocessing import pad_both_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f548bb5e-ceee-49ec-a819-4ffe4bc5c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# adds padding to sentences, on the left and on the right\n",
    "# train contains ngrams (if n=3, train contains 1-grams, 2-grams and 3-grams)\n",
    "# vocab contains padded and flattened sentences\n",
    "temp_data_path = Path('data/ka.txt')\n",
    "n = 3\n",
    "train, vocab = padded_everygram_pipeline(n, tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0d069a-4a32-431f-a933-f5e48fc1aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE, Laplace\n",
    "# model = MLE(n)\n",
    "# TODO: compare different lm models (laplace seems better than mle)\n",
    "model = Laplace(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa340866-3682-4965-914b-ebc7e76e0f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 470950 items>\n",
      "<NgramCounter with 3 ngram orders and 25665363 ngrams>\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, vocab)\n",
    "print(model.vocab)\n",
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3422fc6-7e0b-4913-be59-e2f9f0cb0653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ეროვნული', 'სააგენტო', ':', '\"', 'ახალციხეში', 'აღმოჩენილი', 'შენობა', 'არა', 'საკულტო', 'ნაგებობა', ',', 'არამედ', 'საყოფაცხოვრებო', 'დანიშნულების', 'იყო', '\"')\n",
      "('ეროვნული', 'სააგენტო', ':', '\"', 'ახალციხეში', 'აღმოჩენილი', 'შენობა', 'არა', 'საკულტო', 'ნაგებობა', ',', 'არამედ', 'საყოფაცხოვრებო', 'დანიშნულების', 'იყო', '\"', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "tst_sent = next(generate_tokenized_sentences(temp_data_path))\n",
    "print(model.vocab.lookup(tst_sent))\n",
    "print(model.vocab.lookup(tst_sent + ['salfknoonqwf'])) # check unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e1d93e-d8a5-420b-be79-e6909f433886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add more visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b2749a6-80ba-423f-996f-23c541928ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sentence(model, num_words=20, text_seed=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max num of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "\n",
    "    for token in model.generate(num_words, text_seed=text_seed, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "\n",
    "    return detokenize(text_seed + content) if text_seed else detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a92ebf86-6435-492d-9b83-0d352f28fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ეს არის ფილმი, უკლებრივ არის შედევრი!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, num_words=20, text_seed=['ეს', 'არის'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb1d37f-b1e3-40e8-b1e8-6ec80a82d4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ბ.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random sentence\n",
    "generate_sentence(model, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccad92-3881-4f8d-8436-dbc1a678d082",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c11e989c-e519-4f31-89bf-d09ba2e7845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ngram_dir = Path(\"models/n-gram\")\n",
    "ngram_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fef2dd-2704-44bf-b01e-8145548c987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(ngram_dir / 'ngram.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "548246e9-50a5-49eb-aeb5-ea0221dff621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(ngram_dir / 'ngram.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f5779e8-7201-4c95-aa84-c089b265107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ეს არის ფილმი, უკლებრივ არის შედევრი!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickled model generates the same sentence when using the same parameters.\n",
    "generate_sentence(model_loaded, num_words=20, text_seed=['ეს', 'არის'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a20a009-cc89-4e38-a411-7a61e3664f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ჩემი ხატია სამშობლო პირველად გამოჩნდა წარწერა \" დავითი, - ამის შესახებ საგამოძიებო კომისიის წევრები აცხადებენ, რომ გურამ ვერულიძე, ტენდერში გამარჯვებული მეწარმეებისაგან რეგულარულად იღებდა ქრთამს.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model_loaded, num_words=100, text_seed=['ჩემი', 'ხატია', 'სამშობლო'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bafe184-cc76-4bd3-a5ea-74f74411f143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ქართველი ერი ამას არ აიტანს მას სართულებზე.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickled model generates the same sentence when using the same parameters.\n",
    "generate_sentence(model_loaded, num_words=20, text_seed=['ქართველი', 'ერი', 'ამას', 'არ', 'აიტანს'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406e5952-ac39-49d1-a313-bb105bf25cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ავადმყოფი ერია ქართველები მიჩვეული ვართ ყველაფრის ისე გაგებას, ბრმები დანახვას, პარალიზებული ადამიანები.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickled model generates the same sentence when using the same parameters.\n",
    "generate_sentence(model_loaded, num_words=20, text_seed=['ავადმყოფი', 'ერია', 'ქართველები'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeeec50e-595f-48be-a54f-0dde283fb0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'გიორგი სახლში ტაქსით წამოვედი.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model_loaded, num_words=100, text_seed=['გიორგი', 'სახლში'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57cb1b4c-97f9-40c5-8989-a3852dfa34b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'რა გინდა მოსკოვში?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model_loaded, num_words=100, text_seed=['რა', 'გინდა'], random_seed=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b8f13df-ebe2-4c26-95bd-87b62deeff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "გიორგი სახლში გარდაცვლილი იპოვეს.\n",
      "გიორგი სახლში უსაფრთხოებას 7 მარტის ღამიდან იცავენ.\n",
      "გიორგი სახლში \" დაიხურება.\n",
      "გიორგი სახლში ფეხით მაცილებს.\n",
      "გიორგი სახლში შეხვდებით, გადამეტებული აქტიურობა, თუკი თქვენს ხელისგულზე ორზე მეტი.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_sentence(model_loaded, num_words=100, text_seed=['გიორგი', 'სახლში']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf5b426-6035-4b74-8df0-a81d5259af5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1912',\n",
       " 'წლის',\n",
       " 'მარტში',\n",
       " 'ნეაპოლის',\n",
       " 'ნავსადგურში',\n",
       " 'დიდი',\n",
       " 'საოკეანო',\n",
       " 'გემის',\n",
       " 'გადმოტვირთვისას',\n",
       " 'მოხდა',\n",
       " 'ერთი',\n",
       " 'უცნაური',\n",
       " 'უბედური',\n",
       " 'შემთხვევა',\n",
       " ',',\n",
       " 'რომლის',\n",
       " 'თაობაზე',\n",
       " 'გაზეთებმა',\n",
       " 'დაწვრილებით',\n",
       " ',',\n",
       " 'მაგრამ',\n",
       " 'ძალზე',\n",
       " 'შელამაზებული',\n",
       " 'ცნობები',\n",
       " 'გამოაქვეყნეს',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path = Path('data/ka_nse_test.txt')\n",
    "test_tokenized_sentences = [sent for sent in generate_tokenized_sentences(test_data_path)]\n",
    "test_tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b99741f-7e66-460c-86c3-a836fba1f82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128050it [00:39, 3251.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, test_tokenized_sentences)\n",
    "perplexities = []\n",
    "\n",
    "for test in tqdm(test_data):\n",
    "    perplexities.append(model.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f28a8d41-1615-4c6e-8f2f-5e3670b60023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity: 11146.680899092791\n",
      "\n",
      "Non-trivial sentences with lowest perplexity:\n",
      "- არა , არა , არა , არა , არა !\n",
      "\tScore: 441.9904338589656\n",
      "- რა თქმა უნდა , რა თქმა უნდა , თქვა დედამ .\n",
      "\tScore: 533.3392371147456\n",
      "- რა თქმა უნდა , ვიცი , რომ მდიდარი ხარ .\n",
      "\tScore: 594.9932345812091\n",
      "- რა თქმა უნდა , არა , - მივუგე მე .\n",
      "\tScore: 603.2878588405559\n",
      "- არა , სერ , რა თქმა უნდა , არა !\n",
      "\tScore: 619.0374309955182\n",
      "\n",
      "Non-trivial sentences with highest perplexity:\n",
      "aber statt dessen , wie nutzlos müht er sich ab ; immer noch zwängt er sich durch die gemächer des innersten palastes ; niemals wird er sie überwinden ; und gelänge ihm dies , nichts wäre gewonnen ; die treppen hinab müßte er sich kämpfen ; und gelänge ihm dies , nichts wäre gewonnen ; die höfe wären zu durchmessen ; und nach den höfen der zweite umschließende palast ; und wieder treppen und höfe ; und wieder ein palast ; und so weiter durch jahrtausende ; und stürzte er endlich aus dem äußersten tor - aber niemals , niemals kann es geschehen - liegt erst die residenzstadt vor ihm , die mitte der welt , hochgeschüttet voll ihres bodensatzes .\n",
      "\tScore: 456520.6148791611\n",
      "und vor der ganzen zuschauerschaft seines todes - alle hindernden wände werden niedergebrochen und auf den weit und hoch sich schwingenden freitreppen stehen im ring die großen des reichs - vor allen diesen hat er den boten abgefertigt .\n",
      "\tScore: 349225.04197686625\n",
      "der türhüter stellt öfters kleine verhöre mit ihm an , fragt ihn über seine heimat aus und nach vielem andern , es sind aber teilnahmslose fragen , wie sie große herren stellen , und zum schlusse sagt er ihm immer wieder , daß er ihn noch nicht einlassen könne .\n",
      "\tScore: 347540.7202903902\n",
      "der bote hat sich gleich auf den weg gemacht ; ein kräftiger , ein unermüdlicher mann ; einmal diesen , einmal den andern arm vorstreckend schafft er sich bahn durch die menge ; findet er widerstand , zeigt er auf die brust , wo das zeichen der sonne ist ; er kommt auch leicht vorwärts , wie kein anderer .\n",
      "\tScore: 329770.06132356916\n",
      "\" solche schwierigkeiten hat der mann vom lande nicht erwartet ; das gesetz soll doch jedem und immer zugänglich sein , denkt er , aber als er jetzt den türhüter in seinem pelzmantel genauer ansieht , seine große spitznase , den langen , dünnen , schwarzen tartarischen bart , entschließt er sich doch lieber zu warten , bis er die erlaubnis zum eintritt bekommt .\n",
      "\tScore: 303180.37321662344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f'Average perplexity: {np.mean(perplexities)}\\n')\n",
    "\n",
    "ids_sorted = np.argsort(perplexities)\n",
    "print(f'Non-trivial sentences with lowest perplexity:')\n",
    "p = 0\n",
    "for i in range(99999999):\n",
    "    ii = ids_sorted[i]\n",
    "    if len(test_tokenized_sentences[ii]) > 10:\n",
    "        p += 1\n",
    "        print(f'{\" \".join(test_tokenized_sentences[ii])}\\n\\tScore: {perplexities[ii]}')\n",
    "    if p >= 5:\n",
    "        break\n",
    "\n",
    "print(f'\\nNon-trivial sentences with highest perplexity:')\n",
    "p = 0\n",
    "for i in range(99999999):\n",
    "    ii = ids_sorted[-i]\n",
    "    if len(test_tokenized_sentences[ii]) > 10:\n",
    "        p += 1\n",
    "        print(f'{\" \".join(test_tokenized_sentences[ii])}\\n\\tScore: {perplexities[ii]}')\n",
    "    if p >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5737083-a031-4721-8c4f-ad5cb7b688b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproj",
   "language": "python",
   "name": "nlpproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
