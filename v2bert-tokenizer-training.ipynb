{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ae68e1-21c5-4951-9022-05be8a3d5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0f195b-b280-4ec5-9b48-fc13adfbf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_base = Path('./polished/models/v2bert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbdf376-b12b-40e6-8af4-56cca3903ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './no_en_data/ka_nse_train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25b6c-8a52-441f-8b1e-c5ce1c784235",
   "metadata": {},
   "source": [
    "### Train sub-word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be291f8-9c4a-4a66-9504-3919513def37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['polished/models/v2bert/wordpiece/vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                      strip_accents=True, lowercase=False)\n",
    "\n",
    "wb_tokenizer.train(data_file,\n",
    "                   vocab_size=30000, min_frequency=5,\n",
    "                   special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "wb_tokenizer.save_model(str(out_base / 'wordpiece'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c19b72e-30ea-42c9-a9ba-baaa7a5c8755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_tokenizer.encode('Hi broz').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c23a11-5c04-4730-a10b-9b66c566bdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['შემომ', '##ეჭ', '##ამა']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_tokenizer.encode('შემომეჭამა').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667b9431-d763-4b81-b453-22d2694658e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', 'დიალოგის', 'მაგალითი', '.', 'მაგარია', '?', 'ვნახოთ', 'აბა', '[', 'ლოლ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_tokenizer.encode('-დიალოგის მაგალითი. მაგარია? ვნახოთ აბა [ლოლ').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66e7d3-3982-4283-83b7-d67871152891",
   "metadata": {},
   "source": [
    "### Create BertTokenizer which is now aware of special token meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e327cd81-21ce-4bc0-bb75-3e54183f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(str(out_base / 'wordpiece' / \"vocab.txt\"),\n",
    "                              do_lower_case=False, do_basic_tokenize=True, \n",
    "                              bos_token='[CLS]', \n",
    "                              eos_token='[SEP]', sep_token='[SEP]', \n",
    "                              cls_token='[CLS]', unk_token='[UNK]', \n",
    "                              pad_token='[PAD]', mask_token='[MASK]',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5cbd2f-c80b-4a9d-9747-807dae38c754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('polished/models/v2bert/berttokenizer/tokenizer_config.json',\n",
       " 'polished/models/v2bert/berttokenizer/special_tokens_map.json',\n",
       " 'polished/models/v2bert/berttokenizer/vocab.txt',\n",
       " 'polished/models/v2bert/berttokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(str(out_base / 'berttokenizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f7011f-7f5a-4fb8-b7fb-c2148c8d59f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 6399, 1909, 2096, 3], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('შემომეჭამა')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3fab6-d504-4880-9f49-b8c6cf97bf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
